{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Fintune Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- References:\n",
    "    - [Basic intro](https://platform.openai.com/docs/guides/fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os,sys\n",
    "sys.path.insert(0,'../libs')\n",
    "from oai_fintune_utils import load_jsonl,check_format_errors,token_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "env_path = '../../.env'\n",
    "load_dotenv(dotenv_path=env_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First validate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder='/data/home/xiong/data/Fund/CSR'\n",
    "fintune_data_folder=os.path.join(data_folder,'Fintuning_data','Monetary')\n",
    "train_path = os.path.join(fintune_data_folder,'train_mon_stance.jsonl')\n",
    "test_path = os.path.join(fintune_data_folder,'test_mon_stance.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 462\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are an experience macroeconomist from IMF. \\nGiven a piece of text concerning a particular country in a given year expressing the views of IMF staff, complete the following two tasks. \\n\\nFirst, classify the country\\'s recent or current monetary policy stance as described in the text into \\n**restrictive/neutral/accommodative/unclear/irrelevant**; \\nif it discusses monetary policy but the specific stance is not clear, assign unclear; \\nif it does not discuss monetary policy, assign irrelevant. \\n\\nSecond, classify the IMF staff\\'s recommended or planned near-future (next year) direction of change in monetary policy stance \\nas described in the text into **tightening/tightening bias/no change/loosening bias/loosening/unclear/irrelevant**; \\nif it discusses monetary policy stance but the direction of change is not clear, assign no change; \\nif it does not discuss monetary policy stance, assign unclear (if it discusses monetary policy) \\nor irrelevant (if it does not discuss monetary policy). \\n\\nReturn a JSON dict without additional texts as follows: \\n```json\\n{\"stance_current\": \"<stance_current>\", \\n\"stance_future\": \"<stance_future>\"}\\n```\\n'}\n",
      "{'role': 'user', 'content': 'Country: Tunisia; Year: 2015\\nText:\\n60. Monetary policy should remain prudent. Staff welcomes the recent move toward positive real interest rates and the authoritiesâ€™ readiness to raise interest rates should inflationary pressures increase. The monetary policy transmission mechanism would be greatly enhanced by removing the cap on lending rates and establishing a lender of last resort mechanism.'}\n",
      "{'role': 'assistant', 'content': \"{'stance_current': 'restrictive', 'stance_future': 'tightening bias'}\"}\n",
      "No errors found\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 330, 1410\n",
      "mean / median: 524.7467532467532, 465.5\n",
      "p5 / p95: 375.1, 770.4000000000002\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 15, 19\n",
      "mean / median: 16.738095238095237, 17.0\n",
      "p5 / p95: 16.0, 18.0\n",
      "\n",
      "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n",
      "\n",
      "\n",
      "Dataset has ~242433 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~727299 tokens\n"
     ]
    }
   ],
   "source": [
    "#### run checks\n",
    "data_path = train_path\n",
    "sample_dataset = load_jsonl(data_path,verbose=True)\n",
    "check_format_errors(sample_dataset)\n",
    "token_analysis(sample_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a training job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API is working properly.\n"
     ]
    }
   ],
   "source": [
    "## create clinet and test api key\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "## run one test \n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant\"},\n",
    "    {\"role\": \"user\", \"content\": 'say : api working properly'},\n",
    "]\n",
    "## just run one test, make sure the api works \n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\", \n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path exists\n",
      "file-DzmK6c28Ra77B9FgKfhb7uUC\n",
      "path exists\n",
      "file-nVDsECpM6EEwiFYkVu2RIjNt\n"
     ]
    }
   ],
   "source": [
    "## upload training data\n",
    "if os.path.exists(data_path):\n",
    "    print('path exists')\n",
    "    file = client.files.create(\n",
    "      file=open(data_path, \"rb\"),\n",
    "      purpose=\"fine-tune\"\n",
    "    )\n",
    "    print(file.id)\n",
    "else:\n",
    "    print('path does not exists')\n",
    "\n",
    "## upload validateion  data\n",
    "if os.path.exists(test_path):\n",
    "    print('path exists')\n",
    "    eval_file = client.files.create(\n",
    "      file=open(test_path, \"rb\"),\n",
    "      purpose=\"fine-tune\"\n",
    "    )\n",
    "    print(eval_file.id)\n",
    "else:\n",
    "    print('path does not exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FineTuningJob = client.fine_tuning.jobs.create(\n",
    "    training_file=file.id,  # Specify the training file ID\n",
    "    validation_file=eval_file.id,  # Specify the validation file ID\n",
    "    model=\"gpt-4o-mini-2024-07-18\",  # Specify the model to use\n",
    "    hyperparameters={\n",
    "        \"n_epochs\": \"auto\",     #2,  # Specify the number of epochs for training\n",
    "        \"batch_size\":\"auto\",\n",
    "        \"learning_rate_multiplier\":\"auto\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-avBwxJWen5OjjFoYDev1D83g', created_at=1729487540, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-3AqEuQZh0o1lNQSUihUxYYSd', result_files=[], seed=116659446, status='validating_files', trained_tokens=None, training_file='file-DzmK6c28Ra77B9FgKfhb7uUC', validation_file='file-nVDsECpM6EEwiFYkVu2RIjNt', estimated_finish=None, integrations=[], user_provided_suffix=None)\n"
     ]
    }
   ],
   "source": [
    "print(FineTuningJob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(id, ftevent-t5acFCgKyim715ihy4b6AZ2g)</td>\n",
       "      <td>(created_at, 1729487540)</td>\n",
       "      <td>(level, info)</td>\n",
       "      <td>(message, Validating training file: file-DzmK6...</td>\n",
       "      <td>(object, fine_tuning.job.event)</td>\n",
       "      <td>(data, {})</td>\n",
       "      <td>(type, message)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(id, ftevent-Vk9tQtXTIK6P8zeF2jI8W9za)</td>\n",
       "      <td>(created_at, 1729487540)</td>\n",
       "      <td>(level, info)</td>\n",
       "      <td>(message, Created fine-tuning job: ftjob-avBwx...</td>\n",
       "      <td>(object, fine_tuning.job.event)</td>\n",
       "      <td>(data, {})</td>\n",
       "      <td>(type, message)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        0                         1  \\\n",
       "0  (id, ftevent-t5acFCgKyim715ihy4b6AZ2g)  (created_at, 1729487540)   \n",
       "1  (id, ftevent-Vk9tQtXTIK6P8zeF2jI8W9za)  (created_at, 1729487540)   \n",
       "\n",
       "               2                                                  3  \\\n",
       "0  (level, info)  (message, Validating training file: file-DzmK6...   \n",
       "1  (level, info)  (message, Created fine-tuning job: ftjob-avBwx...   \n",
       "\n",
       "                                 4           5                6  \n",
       "0  (object, fine_tuning.job.event)  (data, {})  (type, message)  \n",
       "1  (object, fine_tuning.job.event)  (data, {})  (type, message)  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List up to 10 events from a fine-tuning job\n",
    "res = client.fine_tuning.jobs.list_events(fine_tuning_job_id=FineTuningJob.id, limit=10)\n",
    "df_res = pd.DataFrame(res.data)\n",
    "df_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
