{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ephemeral/home/xiong/miniconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "sys.path.insert(0,'../../libs')\n",
    "import openai\n",
    "from llm_utils import BSAgent\n",
    "from data_utils import train_val_test_split,load_split_climate_data\n",
    "from utils import download_hf_model\n",
    "import pandas as pd\n",
    "import re,json,copy\n",
    "from tqdm import tqdm\n",
    "from prompts import short_cot_pt,short_cot_pt_2label,long_cot_pt,long_cot_pt_2label,long_fewshotcot_pt_2label\n",
    "import pprint\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up async process\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "nest_asyncio.apply()\n",
    "from llm_utils_async import AsyncBSAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "env_path = '../../../.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables. Please check your .env file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download all models for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define output data model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClimateClassification(BaseModel):\n",
    "    justification: str\n",
    "    classification: Literal[\"favorable\", \"unfavorable\", \"neutral\"]\n",
    "\n",
    "class ClimateClassification_2label(BaseModel):\n",
    "    justification: str\n",
    "    classification: Literal[\"favorable\", \"unfavorable\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define classification function ; sync and async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_climate_classifications(agent, dataset, prompt_template):\n",
    "    results = []\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        structured_prompt = copy.deepcopy(prompt_template)\n",
    "        structured_prompt['user'] = structured_prompt['user'].format(PARAGRAPH=dataset.iloc[i].paragraph)\n",
    "        try:\n",
    "            response = agent.get_response_content(prompt_template=structured_prompt)\n",
    "            response = agent.parse_load_json_str(response)\n",
    "            results.append({\n",
    "                'paragraph': dataset.iloc[i].paragraph,\n",
    "                'true_label': dataset.iloc[i].label,\n",
    "                'predicted_label': response.get('classification'),\n",
    "                'justification': response.get('justification')\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {i}: {str(e)}\")\n",
    "            results.append({\n",
    "                'paragraph': dataset.iloc[i].paragraph,\n",
    "                'true_label': dataset.iloc[i].label,\n",
    "                'predicted_label': None,\n",
    "                'justification': f\"Error: {str(e)}\"\n",
    "            })\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/ephemeral/home/xiong/data/Fund/Climate'\n",
    "# data_path = os.path.join(data_folder,'Climate training paragraphs.csv')\n",
    "# ds = load_split_climate_data(data_path,merge_neutral=True,verbose=True)\n",
    "# ds['test'].to_csv(data_folder+'/test.csv')\n",
    "# ds['validation'].to_csv(data_folder+'/validation.csv')\n",
    "# ds['train'].to_csv(data_folder+'/train.csv')\n",
    "test_data = pd.read_csv(data_folder+'/test.csv')\n",
    "val_data = pd.read_csv(data_folder+'/validation.csv')\n",
    "train_data = pd.read_csv(data_folder+'/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup experiment scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model names, prompt templates, and API configuration\n",
    "model_name_list = [\n",
    "    'anthropic/claude-3-7-sonnet-latest', # Claude 3 Sonnet\n",
    "    'google/gemini-2.0-flash'   # Google Gemini Flash\n",
    "]\n",
    "\n",
    "prompt_template_list = [long_fewshotcot_pt_2label, long_cot_pt_2label, short_cot_pt_2label]\n",
    "# Create dictionary with model names as keys and prompt templates as values\n",
    "experiment_dict = {}\n",
    "for model in model_name_list:\n",
    "    # Extract model name after the '/' character\n",
    "    model_short_name = model.split('/')[-1]\n",
    "    # Create nested dictionary for each prompt template with API configuration\n",
    "    experiment_dict[model_short_name] = {\n",
    "        'long_fewshot_cot': long_fewshotcot_pt_2label,\n",
    "        'long_cot': long_cot_pt_2label,\n",
    "        'short_cot': short_cot_pt_2label,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model and initiate llm agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_utils_claude import ClaudeAgent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today? Whether you have a question, need information, or just want to chat, I'm here to help. What's on your mind?\n"
     ]
    }
   ],
   "source": [
    "model_name = 'claude-3-7-sonnet-latest'\n",
    "agent = ClaudeAgent(model=model_name,temperature=0.0,max_tokens=4000,api_key=os.getenv(\"CLAUDE_API_KEY\"))\n",
    "agent.connection_test('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [03:44<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results: claude-3-7-sonnet-latest ; long_fewshot_cot\n",
      "Total samples: 108\n",
      "Successfully processed: 108\n",
      "Validation Accuracy: 79.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [04:25<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results: claude-3-7-sonnet-latest ; long_cot\n",
      "Total samples: 108\n",
      "Successfully processed: 108\n",
      "Validation Accuracy: 78.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [03:05<00:00,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results: claude-3-7-sonnet-latest ; short_cot\n",
      "Total samples: 108\n",
      "Successfully processed: 108\n",
      "Validation Accuracy: 74.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for validation and test sets\n",
    "experiment = experiment_dict[model_name]\n",
    "for promt_type in experiment.keys():\n",
    "    val_results = get_climate_classifications(agent, test_data, experiment[promt_type])\n",
    "    val_results.to_csv(os.path.join( data_folder,'training_eval_results','{}_{}_val_results_v2.csv'.format(model_name,promt_type)))\n",
    "\n",
    "    print(\"\\nValidation Results: {} ; {}\".format(model_name,promt_type))\n",
    "    print(f\"Total samples: {len(val_results)}\")\n",
    "    print(f\"Successfully processed: {len(val_results[val_results.predicted_label.notna()])}\")\n",
    "    val_accuracy = (val_results['true_label'] == val_results['predicted_label']).mean()\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try run with asyc clent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain to me how AI works\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break down how AI works, starting with the core concepts and then diving into some common techniques.  It's a vast field, so I'll try to keep it accessible.\n",
      "\n",
      "**What is AI?**\n",
      "\n",
      "At its most basic, Artificial Intelligence (AI) is about creating computer systems that can perform tasks that typically require human intelligence.  This includes things like:\n",
      "\n",
      "*   **Learning:**  Improving performance over time through experience.\n",
      "*   **Reasoning:**  Drawing conclusions and making decisions based on available information.\n",
      "*   **Problem-solving:**  Finding solutions to complex issues.\n",
      "*   **Perception:**  Interpreting sensory input (like images, sound, and text).\n",
      "*   **Natural Language Understanding:**  Comprehending and generating human language.\n",
      "\n",
      "**The Core Components: Data, Algorithms, and Computation**\n",
      "\n",
      "AI systems rely on three essential pillars:\n",
      "\n",
      "1.  **Data:** AI algorithms need a large amount of data to learn from.  This data can be anything from images and text to sensor readings and financial records.  The quality and quantity of the data significantly impact the AI's performance.\n",
      "\n",
      "2.  **Algorithms:** These are the sets of rules or instructions that tell the AI system how to process data and make decisions.  Different algorithms are suited for different tasks.  Think of them as the \"recipes\" for AI.\n",
      "\n",
      "3.  **Computation:** AI algorithms often require significant processing power to analyze data and learn patterns.  This is where powerful computers, including specialized hardware like GPUs (Graphics Processing Units), come into play.\n",
      "\n",
      "**How AI Systems Learn: Machine Learning**\n",
      "\n",
      "A significant portion of modern AI relies on a subfield called **Machine Learning (ML)**.  Instead of explicitly programming a computer to perform a specific task, ML algorithms learn from data without being explicitly programmed.  Here's a breakdown of the main types of ML:\n",
      "\n",
      "*   **Supervised Learning:**\n",
      "    *   **Concept:** The algorithm is trained on a labeled dataset, meaning each data point has a correct \"answer\" associated with it.\n",
      "    *   **Example:** Training a system to classify images of cats and dogs. The training data would consist of images labeled as either \"cat\" or \"dog.\"  The algorithm learns to associate features (like ear shape, nose size, etc.) with the correct label.\n",
      "    *   **Common Algorithms:** Linear Regression, Logistic Regression, Support Vector Machines (SVMs), Decision Trees, Random Forests, Neural Networks.\n",
      "\n",
      "*   **Unsupervised Learning:**\n",
      "    *   **Concept:** The algorithm is trained on an unlabeled dataset, meaning there are no pre-defined correct answers. The algorithm must discover patterns and relationships in the data on its own.\n",
      "    *   **Example:** Clustering customers based on their purchasing behavior. The algorithm might identify distinct groups of customers with different spending habits without being told what those groups are beforehand.\n",
      "    *   **Common Algorithms:** K-Means Clustering, Hierarchical Clustering, Principal Component Analysis (PCA), Anomaly Detection.\n",
      "\n",
      "*   **Reinforcement Learning:**\n",
      "    *   **Concept:** The algorithm learns by interacting with an environment and receiving rewards or penalties for its actions.  It aims to maximize its cumulative reward over time.\n",
      "    *   **Example:** Training an AI to play a video game. The AI receives positive rewards for scoring points and negative rewards for losing. Through trial and error, it learns the optimal strategies to win the game.\n",
      "    *   **Common Algorithms:** Q-Learning, Deep Q-Networks (DQN), Policy Gradient Methods.\n",
      "\n",
      "**Key AI Techniques:**\n",
      "\n",
      "*   **Neural Networks:** These are a type of machine learning model inspired by the structure of the human brain.  They consist of interconnected nodes (neurons) organized in layers.  Data flows through the network, and the connections between neurons are adjusted during training to improve accuracy.  Deep Learning is a subset of machine learning that uses neural networks with many layers (hence \"deep\"). Deep learning excels at complex tasks like image recognition, natural language processing, and speech recognition.\n",
      "*   **Natural Language Processing (NLP):** Focuses on enabling computers to understand, interpret, and generate human language.  It involves techniques like:\n",
      "    *   **Text analysis:**  Extracting meaning and insights from text.\n",
      "    *   **Sentiment analysis:**  Determining the emotional tone of text.\n",
      "    *   **Machine translation:**  Automatically translating text from one language to another.\n",
      "    *   **Chatbots:**  Developing conversational AI agents that can interact with users.\n",
      "*   **Computer Vision:**  Enables computers to \"see\" and interpret images and videos.  It involves techniques like:\n",
      "    *   **Image recognition:**  Identifying objects in images.\n",
      "    *   **Object detection:**  Locating and identifying multiple objects in an image.\n",
      "    *   **Image segmentation:**  Dividing an image into regions based on their content.\n",
      "*   **Robotics:**  Combines AI with engineering to create intelligent robots that can perform physical tasks.  AI is used for robot control, navigation, and perception.\n",
      "*   **Expert Systems:** Designed to mimic the decision-making abilities of a human expert in a specific domain. They use a knowledge base and inference engine to provide advice and solve problems.\n",
      "\n",
      "**The AI Development Process (Simplified)**\n",
      "\n",
      "1.  **Define the Problem:** Clearly identify the task you want the AI to perform.\n",
      "2.  **Gather Data:** Collect a large and relevant dataset to train the AI model.\n",
      "3.  **Choose an Algorithm:** Select the appropriate machine learning algorithm based on the problem and the data.\n",
      "4.  **Train the Model:**  Feed the data into the algorithm and allow it to learn patterns.\n",
      "5.  **Evaluate the Model:**  Assess the model's performance on a separate test dataset.\n",
      "6.  **Fine-Tune the Model:**  Adjust the model's parameters and retrain it to improve accuracy.\n",
      "7.  **Deploy the Model:**  Integrate the model into a real-world application.\n",
      "8.  **Monitor and Maintain:**  Continuously monitor the model's performance and retrain it as needed to maintain accuracy over time.\n",
      "\n",
      "**Challenges and Considerations:**\n",
      "\n",
      "*   **Bias:** AI models can inherit biases from the data they are trained on, leading to unfair or discriminatory outcomes.\n",
      "*   **Explainability:**  Some AI models, particularly deep learning models, are difficult to understand, making it challenging to explain their decisions.  This is known as the \"black box\" problem.\n",
      "*   **Data Requirements:** Many AI algorithms require vast amounts of data, which can be expensive and time-consuming to collect and prepare.\n",
      "*   **Ethical Concerns:**  AI raises a number of ethical concerns, such as job displacement, privacy violations, and the potential for misuse.\n",
      "*   **Security:** AI systems can be vulnerable to attacks that can compromise their performance or steal sensitive data.\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "AI works by using algorithms to process data and learn from it. Machine learning is a key subfield of AI that enables computers to learn without explicit programming.  AI is a rapidly evolving field with the potential to transform many aspects of our lives, but it is important to be aware of the challenges and ethical considerations associated with its development and deployment.\n",
      "\n",
      "I hope this explanation is helpful. Let me know if you have more specific questions!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Explain how AI works\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
