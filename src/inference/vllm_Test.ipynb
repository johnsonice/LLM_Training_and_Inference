{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference with Vllm Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import transformers\n",
    "import torch , os\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-16 19:13:07 config.py:1218] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 06-16 19:13:07 llm_engine.py:161] Initializing an LLM engine (v0.5.0) with config: model='/root/data/hf_cache/llama-3-8B-Instruct', speculative_config=None, tokenizer='/root/data/hf_cache/llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/root/data/hf_cache/llama-3-8B-Instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-16 19:13:12 model_runner.py:159] Loading model weights took 14.9595 GB\n",
      "INFO 06-16 19:13:13 gpu_executor.py:83] # GPU blocks: 27889, # CPU blocks: 2048\n",
      "INFO 06-16 19:13:17 model_runner.py:878] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-16 19:13:17 model_runner.py:882] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-16 19:13:24 model_runner.py:954] Graph capturing finished in 7 secs.\n",
      "/root/data/hf_cache/llama-3-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# initialize model \n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "model_cache_dir = '/root/data/hf_cache/llama-3-8B-Instruct'\n",
    "llm = LLM(model=model_cache_dir,dtype=torch.float16,tensor_parallel_size=1)\n",
    "vllm_tokenizer = llm.get_tokenizer()\n",
    "print(vllm_tokenizer.name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare prompt for corresponding LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(message,tokenizer,system_prompt=None,chat_history=[],fromat=True):\n",
    "    conversation = []\n",
    "    if system_prompt:\n",
    "        conversation.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    for user, assistant in chat_history:\n",
    "        conversation.extend([{\"role\": \"user\", \"content\": user}, {\"role\": \"assistant\", \"content\": assistant}])\n",
    "    conversation.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    if fromat:\n",
    "        conversation = tokenizer.apply_chat_template(conversation,tokenize=False) \n",
    "    \n",
    "    return conversation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You name is Llama 3 bot. You are a friendly chatbot who always provide very very short answer.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is your name?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"What is your name?\",\n",
    "    \"The president of the United States is who ?\",\n",
    "    \"What are the pros/cons of ChatGPT vs Open Source LLMs?\",\n",
    "    \"Write an email to a new client to offer a subscription for a paper supply for 1 year.\",\n",
    "    \"I have $10,000 USD for investment. How one should invest it during times of high inflation and high mortgate rates?\",\n",
    "    \"Write a function in python that calculates the square of a sum of two numbers.\",\n",
    "]\n",
    "system_prompt = \"You name is Llama 3 bot. You are a friendly chatbot who always provide very very short answer.\"\n",
    "formated_prompts = [create_prompt(text, vllm_tokenizer,system_prompt) for text in prompts]\n",
    "print(formated_prompts[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 6/6 [00:01<00:00,  4.34it/s, Generation Speed: 140.51 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \"<|start_header_id|>assistant<|end_header_id|>\\n\\nI'm Llama 3!\"\n",
      "performance metrics: RequestMetrics(arrival_time=1718565243.5628803, last_token_time=1718565243.5628803, first_scheduled_time=1718565243.5764158, first_token_time=1718565243.6157765, time_in_queue=0.013535499572753906, finished_time=1718565243.7696753)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# perform the inference\n",
    "sampling_params = SamplingParams( temperature=0.6,\n",
    "                                    top_p=0.9,\n",
    "                                    max_tokens=128,\n",
    "                                    stop_token_ids=[vllm_tokenizer.eos_token_id, \n",
    "                                                    vllm_tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")],\n",
    "                                    skip_special_tokens=True )\n",
    "outputs = llm.generate(formated_prompts, sampling_params)\n",
    "\n",
    "# print outputs\n",
    "for output in outputs[:1]:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Generated text: {generated_text!r}\")\n",
    "    print(f\"performance metrics: {output.metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vllm_generate_benchmark():\n",
    "    outputs = llm.generate(formated_prompts, sampling_params,use_tqdm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used: 14.21\n"
     ]
    }
   ],
   "source": [
    "# Running the timeit 10 times\n",
    "execution_time = timeit.repeat(stmt=vllm_generate_benchmark, repeat=10, number=1)\n",
    "print(\"Time used: {:.2f}\".format(sum(execution_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare it is transformer pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ec08017872429fb065612e3c602c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "## load model \n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_cache_dir,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}, ## load with bf16\n",
    "    device_map=torch.device(\"cuda:2\"), ## put it on gpu1 \n",
    ")\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run one test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'Llama 3!'}\n"
     ]
    }
   ],
   "source": [
    "messages = [create_prompt(text, pipeline.tokenizer,system_prompt,fromat=False) for text in prompts]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=128,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=pipeline.tokenizer.eos_token_id)\n",
    "\n",
    "print(outputs[0][0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_generate_benchmark():\n",
    "    outputs = pipeline(\n",
    "            messages,\n",
    "            max_new_tokens=128,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=pipeline.tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used: 48.54\n"
     ]
    }
   ],
   "source": [
    "# Running the timeit 10 times\n",
    "execution_time = timeit.repeat(stmt=hf_generate_benchmark, \n",
    "                                repeat=10, number=1)\n",
    "print(\"Time used: {:.2f}\".format(sum(execution_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
